[BERT](https://github.com/google-research/bert), or Bidirectional Encoder Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of 
Natural Language Processing (NLP) tasks including with Bulgarian words and text.
BERT (from Google) released with the paper [BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.](https://arxiv.org/abs/1810.04805)
