[BERT](https://github.com/google-research/bert), or Bidirectional Encoder Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of 
Natural Language Processing (NLP) tasks including with Bulgarian words and text.
BERT (from Google) released with the paper [BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.](https://arxiv.org/abs/1810.04805)
NLP researchers from HuggingFace made a [PyTorch version of BERT available](https://github.com/huggingface/pytorch-pretrained-BERT) which is compatible with our pre-trained checkpoints and is able to reproduce our results. Sosuke Kobayashi also made a Chainer version of BERT available (Thanks!) 
